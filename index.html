<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jaewoo (Jeffrey) Heo</title>

    <meta name="author" content="Jaewoo Heo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jaewoo (Jeffrey) Heo
                </p>
                <p>Hey! I'm Jaewoo (Jeffrey), an ML researcher at the Stanford <a href="https://marvl.stanford.edu/index.html">MARVL</a> (Medical AI and ComputeR Vision Lab) where I am advised by Professor <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>. I am also a part of <a href="https://svl.stanford.edu/">Stanford Vision and Learning</a> Lab, where I work with Professor <a href="https://scholar.google.com/citations?user=rDfyQnIAAAAJ&hl=en">Fei-Fei Li</a> and Professor <a href="https://stanford.edu/~eadeli/">Ehsan Adeli</a> in the <a href="https://med.stanford.edu/pacresearch.html"> Partner in AI-Assisted Care</a> (PAC) group.
                </p>
                <p>
                  I received my B.S. in Computer Science with Honors from Stanford University in 2024, and my M.S in Computer Science from Stanford University in 2025. Throughout my career in ML, I've been primarily involved with computer vision and medical AI research. My recent works in query-agnostic deformable cross attention, score distillation sampling with diffusion models, long-form video understanding, and VLM for reliable data generation aim to develop machines with enhanced spatiotemporal understanding of our world. 
                </p>
                <p>
                  In my free time, I enjoy playing the guitar, writing music, playing tennis, and watching soccer. I am a huge fan of The Beatles, Pink Floyd, and Billy Joel. I used to never be able to listen to music while coding, but I've recently developed a knack for putting on some bossa nova while working.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jeffheo@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Jaewoo_Jeffrey_Heo_Resume_2025_CV_v1.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=uAmuZXkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jeffheo/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/mypic.jpeg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/mypic.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p style="text-align: left; font-style: italic;">
                  "Understanding the problem is half the solution"
                </p>
                <p>
                  My research interests lie in computer vision and deep learning for perception and spatiotemporal understanding to make meaningful inferences of the world. Specifically, I'm passionate about applied ML research particularly in the fields of robotics and autonomous vehicles. My work aims to bridge the gap between technical innovation and real-world application. 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

            <tr onmouseout="surgbench_stop()" onmouseover="surgbench_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/surgbench.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Systematic Evaluation of Large Vision-Language Models for Surgical Artificial Intelligence</span>
                <br>
                Anita Rau, Mark Endo, Josiah Aklilu, Jaewoo Heo, Khaled Saab, Alberto Paderno, Jeffrey Jopling, F. Christopher Holsinger, Serena Yeung-Levy
                <br>
                <em>arXiv</em>, 2025
                <br>
                <a href="https://anitarau.github.io/surg-vlms-eval/">project page</a>
                /
                <a href="https://arxiv.org/abs/2504.02799">arXiv</a>
                /
                code (coming soon)
                <p>
                  Evaluated 11 state-of-the-art VLMs across 17 surgical AI tasks using 13 datasets, demonstrating VLMs' superior generalizability compared to supervised models when deployed outside their training environments 
                </p>
              </td>
            </tr>

            <tr onmouseout="deforhmr_stop()" onmouseover="deforhmr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='deforhmr_image'>
                    <img src='images/deforhmr.png' width=100%>
                  </div>
                  <img src='images/deforhmr' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery</span>
                <br>
                Jaewoo Heo, George Hu, Zeyu Wang, Serena Yeung-Levy
                <br>
                <em>3DV</em>, 2025 <font color="red"><strong>(accepted)</strong></font>
                <br>
                <a href="https://jeffheo.github.io/deforhmr/">project page</a>
                /
                <a href="https://openreview.net/forum?id=j6AIRRxwbU">3DV'25 Camera-ready</a>
                /
                code (coming soon)
                <p>
                  Proposes a novel query-agnostic deformable cross-attention mechanism that allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner. Achieves SOTA performance on 3D human mesh recovery benchmarks 3DPW and RICH.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="diffopt_stop()" onmouseover="diffopt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/diffopt.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Motion Diffusion-Guided 3D Global HMR from a Dynamic Camera
                </span>
                <br>
                Jaewoo Heo, Kuan-Chieh Wang, Karen Liu, Serena Yeung-Levy
                <br>
                <em>arXiv</em>, 2024 
                <br>
                <a href="https://sites.google.com/view/diffopt/home">project page</a>
                /
                <a href="https://arxiv.org/abs/2411.10582">arXiv</a>
                /
                code (coming soon)
                <p>
                  A 3D global HMR model that leverages the motion diffusion model (MDM) as a prior of coherent human motion. The model is robust to dynamic camera motion and long videos. 
                </p>
              </td>
            </tr>
            
            <tr onmouseout="apu_stop()" onmouseover="apu_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/apu.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models
                </span>
                <br>
                Laura Bravo-SÃ¡nchez, Jaewoo Heo, Zhenzhen Weng, Kuan-Chieh Wang, Serena Yeung-Levy
                <br>
                <em>CVPR</em>, 2025 <font color="red"><strong>(submitted)</strong></font>
                <br>
                <a href="https://laubravo.github.io/apu_website/">project page</a>
                /
                <a href="https://arxiv.org/abs/2410.00309">arXiv</a>
                /
                code (coming soon)
                <p>
                  Proposes a novel data generation method for close interactions that leverages noisy automatic annotations to scale data acquisition, producing pseudo-ground truth meshes from in-the-wild images.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="neuhmr_stop()" onmouseover="neuhmr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/neuhmr.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">NeuHMR: Neural Rendering-Guided Human Motion Reconstruction
                </span>
                <br>
                Tiange Xiang, Kuan-Chieh Wang, Jaewoo Heo, Ehsan Adeli, Serena Yeung-Levy, Scott Delp, Li Fei-Fei
                <br>
                <em>3DV</em>, 2025 <font color="red"><strong>(accepted)</strong></font>
                <br>
                project page (coming soon)
                /
                arXiv (coming soon)
                /
                code (coming soon)
                <p>
                  Rethinks the dependency on the 2D key point fitting paradigm and presents NeuHMR, an optimization-based mesh recovery framework based on recent advances in neural rendering (NeRF).
                </p>
              </td>
            </tr>
            
          
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  I referred to this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a> to build my own.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
