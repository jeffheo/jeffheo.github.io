<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jaewoo (Jeffrey) Heo</title>

    <meta name="author" content="Jaewoo Heo">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jaewoo (Jeffrey) Heo
                </p>
                <p>Hey! I'm Jaewoo (Jeffrey), a coterminal master's (MS) student in Computer Science at Stanford University. I'm a research assistant at the Stanford <a href="https://marvl.stanford.edu/index.html">MARVL</a> (Medical AI and ComputeR Vision Lab) where I am advised by Professor <a href="https://ai.stanford.edu/~syyeung/">Serena Yeung-Levy</a>. I am also a part of <a href="https://svl.stanford.edu/">Stanford Vision and Learning</a> Lab, where I work with Professor <a href="https://stanford.edu/~eadeli/">Ehsan Adeli</a> in the <a href="https://med.stanford.edu/pacresearch.html"> Partner in AI-Assisted Care</a> (PAC) group.
                </p>
                <p>
                  I received my B.S. in Computer Science with Honors from Stanford University in 2024. During my studies at Stanford, I've been primarily involved with computer vision and medical AI research. My recent works in query-agnostic deformable cross attention, diffusion models as prior, and VLM for reliable data generation aim to enhance our computational understanding of human behavior and interaction in 3D. 
                </p>
                <p>
                  In my free time, I enjoy playing the guitar, writing music, playing tennis, and watching soccer. I am a huge fan of The Beatles, Pink Floyd, and Billy Joel. I used to never be able to listen to music while coding, but I've recently developed a knack for putting on some bossa nova while working.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jeffheo@stanford.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/Jaewoo_Jeffrey_Heo_Resume_2024_v4.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=uAmuZXkAAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jeffheo/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/profile.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p style="text-align: left; font-style: italic;">
                  "Understanding the problem is half the solution"
                </p>
                <p>
                  My research interests lie in computer vision and deep learning for understanding human behavior, motion, and interaction, particularly in healthcare contexts. I am driven by the challenge of creating AI tools that can understand humans in 4D (3D spatial + 1D temporal) to make meaningful inferences about the human condition. My work aims to bridge the gap between technical innovation and real-world application, integrating novel algorithms with domain-specific challenges in healthcare and beyond. 
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


            <tr onmouseout="deforhmr_stop()" onmouseover="deforhmr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <div class="two" id='deforhmr_image'>
                    <img src='images/deforhmr.png' width=100%>
                  </div>
                  <img src='images/deforhmr' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">DeforHMR: Vision Transformer with Deformable Cross-Attention for 3D Human Mesh Recovery</span>
                <br>
                Jaewoo Heo, George Hu, Zeyu Wang, Serena Yeung-Levy
                <br>
                <em>3DV</em>, 2025 <font color="red"><strong>(accepted)</strong></font>
                <br>
                <a href="https://arxiv.org/abs/2411.11214">project page</a>
                /
                <a href="https://arxiv.org/abs/2411.11214">arXiv</a>
                /
                code (coming soon)
                <p>
                  Proposes a novel query-agnostic deformable cross-attention mechanism that allows the model to attend to relevant spatial features more flexibly and in a data-dependent manner. Achieves SOTA performance on 3D human mesh recovery benchmarks 3DPW and RICH.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="diffopt_stop()" onmouseover="diffopt_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/diffopt.png' width="160">
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Motion Diffusion-Guided 3D Global HMR from a Dynamic Camera
                </span>
                <br>
                Jaewoo Heo, Kuan-Chieh Wang, Karen Liu, Serena Yeung-Levy
                <br>
                <em>arXiv</em>, 2024 
                <br>
                <a href="https://arxiv.org/abs/2411.10582">project page</a>
                /
                <a href="https://arxiv.org/abs/2411.10582">arXiv</a>
                /
                code (coming soon)
                <p>
                  A 3D global HMR model that leverages the motion diffusion model (MDM) as a prior of coherent human motion. The model is robust to dynamic camera motion and long videos. 
                </p>
              </td>
            </tr>
            
            <tr onmouseout="apu_stop()" onmouseover="apu_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/apu.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">Ask, Pose, Unite: Scaling Data Acquisition for Close Interactions with Vision Language Models
                </span>
                <br>
                Laura Bravo-SÃ¡nchez, Jaewoo Heo, Zhenzhen Weng, Kuan-Chieh Wang, Serena Yeung-Levy
                <br>
                <em>CVPR</em>, 2025 <font color="red"><strong>(submitted)</strong></font>
                <br>
                <a href="https://laubravo.github.io/apu_website/">project page</a>
                /
                <a href="https://arxiv.org/abs/2410.00309">arXiv</a>
                /
                code (coming soon)
                <p>
                  Proposes a novel data generation method for close interactions that leverages noisy automatic annotations to scale data acquisition, producing pseudo-ground truth meshes from in-the-wild images.
                </p>
              </td>
            </tr>
            
            <tr onmouseout="neuhmr_stop()" onmouseover="neuhmr_start()">
              <td style="padding:20px;width:25%;vertical-align:middle">
                <div class="one">
                  <img src='images/neuhmr.png' width=100%>
                </div>
              </td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <span class="papertitle">NeuHMR: Neural Rendering-Guided Human Motion Reconstruction
                </span>
                <br>
                Tiange Xiang, Kuan-Chieh Wang, Jaewoo Heo, Ehsan Adeli, Serena Yeung-Levy, Scott Delp, Li Fei-Fei
                <br>
                <em>3DV</em>, 2025 <font color="red"><strong>(accepted)</strong></font>
                <br>
                project page (coming soon)
                /
                arXiv (coming soon)
                /
                code (coming soon)
                <p>
                  Rethinks the dependency on the 2D key point fitting paradigm and presents NeuHMR, an optimization-based mesh recovery framework based on recent advances in neural rendering (NeRF).
                </p>
              </td>
            </tr>
            
          
        
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  I referred to this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a> to build my own.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
